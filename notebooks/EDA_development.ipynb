{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# ðŸ”® Intelligent Predictor PRO++++ â€” EDA Development Notebook\n",
        "\n",
        "**Purpose:** Development and testing environment for Exploratory Data Analysis features\n",
        "\n",
        "**Author:** Data Science Team  \n",
        "**Date:** 2025-10-09  \n",
        "**Version:** 2.0.1\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Table of Contents\n",
        "\n",
        "1. [Setup & Imports](#setup)\n",
        "2. [Data Loading](#data-loading)\n",
        "3. [Data Quality Assessment](#data-quality)\n",
        "4. [Statistical Analysis](#statistical-analysis)\n",
        "5. [Univariate Analysis](#univariate)\n",
        "6. [Bivariate Analysis](#bivariate)\n",
        "7. [Multivariate Analysis](#multivariate)\n",
        "8. [Advanced Visualizations](#advanced-viz)\n",
        "9. [Automated Profiling](#profiling)\n",
        "10. [Dashboard Creation](#dashboards)\n",
        "11. [Export & Reports](#export)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n",
        "\n",
        "Defensive imports: try to use project modules if available, otherwise fall back to inline helpers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === STANDARD LIB ===\n",
        "import os, sys, json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# === DATA ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# === VIZ ===\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    _HAS_SNS = True\n",
        "except Exception:\n",
        "    _HAS_SNS = False\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# === PROJECT MODULES (optional) ===\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "try:\n",
        "    from src.data_processing.file_parser import parse_csv_smart, parse_any\n",
        "except Exception:\n",
        "    parse_csv_smart = None\n",
        "    def parse_any(path: str) -> pd.DataFrame:\n",
        "        if path.lower().endswith('.csv'):\n",
        "            return pd.read_csv(path)\n",
        "        elif path.lower().endswith(('.xlsx', '.xls')):\n",
        "            return pd.read_excel(path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported format: {path}\")\n",
        "try:\n",
        "    from src.data_processing.data_cleaner import clean_data\n",
        "except Exception:\n",
        "    def clean_data(df: pd.DataFrame, remove_duplicates=True, handle_missing='auto', detect_outliers=True) -> pd.DataFrame:\n",
        "        out = df.copy()\n",
        "        if remove_duplicates:\n",
        "            out = out.drop_duplicates()\n",
        "        if handle_missing == 'auto':\n",
        "            num_cols = out.select_dtypes(include=[np.number]).columns\n",
        "            for c in num_cols:\n",
        "                out[c] = out[c].fillna(out[c].median())\n",
        "            cat_cols = out.select_dtypes(include=['object', 'category']).columns\n",
        "            for c in cat_cols:\n",
        "                out[c] = out[c].fillna(out[c].mode().iloc[0] if out[c].mode().size else out[c])\n",
        "        return out\n",
        "try:\n",
        "    from src.data_processing.data_validator import validate_dataframe_for_ml\n",
        "except Exception:\n",
        "    def validate_dataframe_for_ml(df: pd.DataFrame, target: Optional[str]=None) -> Dict[str, Any]:\n",
        "        errors, warnings_ = [], []\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            errors.append('Input is not a DataFrame')\n",
        "        if target and target not in df.columns:\n",
        "            errors.append(f\"Target '{target}' not found\")\n",
        "        if df.empty:\n",
        "            warnings_.append('DataFrame is empty')\n",
        "        return {'is_valid': len(errors)==0, 'errors': errors, 'warnings': warnings_}\n",
        "try:\n",
        "    from src.data_processing.data_profiler import generate_profile\n",
        "except Exception:\n",
        "    def generate_profile(df: pd.DataFrame, title: str='Profile', dark_mode: bool=True, minimal: bool=False) -> str:\n",
        "        html = [f\"<h1>{title}</h1>\", f\"<p>Rows: {len(df):,}, Cols: {df.shape[1]}</p>\"]\n",
        "        html.append(df.head().to_html(index=False))\n",
        "        return \"\\n\".join(html)\n",
        "\n",
        "# === SIMPLE VIZ HELPERS (fallbacks) ===\n",
        "def histogram(df, col, binning_method='fd', title='Histogram', **kwargs):\n",
        "    return px.histogram(df, x=col, nbins=None, title=title, marginal=kwargs.get('marginal'))\n",
        "def scatter(df, x, y, title='Scatter', **kwargs):\n",
        "    return px.scatter(df, x=x, y=y, trendline=kwargs.get('trendline'))\n",
        "def line(df, x, y, title='Line', **kwargs):\n",
        "    fig = px.line(df, x=x, y=y, title=title)\n",
        "    if kwargs.get('show_smoothing'):\n",
        "        w = int(kwargs.get('smoothing_window', 7))\n",
        "        s = df[y].rolling(w).mean()\n",
        "        fig.add_trace(go.Scatter(x=df[x], y=s, name=f\"MA_{w}\"))\n",
        "    return fig\n",
        "def box(df, y=None, x=None, title='Box', **kwargs):\n",
        "    return px.box(df, x=x, y=y, points=kwargs.get('points', 'outliers'), title=title)\n",
        "def violin(df, x=None, y=None, title='Violin', **kwargs):\n",
        "    return px.violin(df, x=x, y=y, box=kwargs.get('box', True), points=kwargs.get('points', 'outliers'), title=title)\n",
        "def correlation_heatmap(df, method='pearson', annotate=False, cluster=False, cmap='RdBu_r', title='Correlation'):\n",
        "    corr = df.select_dtypes(include=[np.number]).corr(method=method)\n",
        "    fig = px.imshow(corr, title=title, color_continuous_scale='RdBu', aspect='auto')\n",
        "    return fig\n",
        "def scatter_3d(df, x, y, z, color=None, title='3D Scatter'):\n",
        "    return px.scatter_3d(df, x=x, y=y, z=z, color=color, title=title)\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid') if hasattr(plt, 'style') else None\n",
        "print(\"âœ… Imports OK | Pandas:\", pd.__version__, \"| NumPy:\", np.__version__, \"| Plotly:\", px.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "helper-functions",
      "metadata": {},
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "helpers",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_info(df: pd.DataFrame, title: str = \"Dataset Info\"):\n",
        "    print(f\"\\n{'='*60}\\nðŸ“Š {title}\\n{'='*60}\")\n",
        "    print(f\"Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "    print(f\"Memory: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
        "    dups = int(df.duplicated().sum())\n",
        "    miss = int(df.isna().sum().sum())\n",
        "    print(f\"Duplicates: {dups:,} ({(dups/len(df)*100 if len(df)>0 else 0):.2f}%)\")\n",
        "    print(f\"Missing: {miss:,} values ({(miss/df.size*100 if df.size>0 else 0):.2f}%)\")\n",
        "    print(\"\\nðŸ“ˆ Data Types:\")\n",
        "    print(df.dtypes.value_counts())\n",
        "    print(\"\\nðŸ” Missing Values by Column:\")\n",
        "    missing = df.isna().sum()\n",
        "    missing = missing[missing > 0].sort_values(ascending=False)\n",
        "    if len(missing) > 0:\n",
        "        for col, count in missing.items():\n",
        "            pct = count / len(df) * 100 if len(df)>0 else 0\n",
        "            print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
        "    else:\n",
        "        print(\"  âœ… No missing values!\")\n",
        "\n",
        "def describe_numeric(df: pd.DataFrame, cols: Optional[List[str]] = None):\n",
        "    if cols is None:\n",
        "        cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if not cols:\n",
        "        return pd.DataFrame()\n",
        "    desc = df[cols].describe().T\n",
        "    desc['skew'] = df[cols].skew(numeric_only=True)\n",
        "    desc['kurtosis'] = df[cols].kurtosis(numeric_only=True)\n",
        "    desc['missing'] = df[cols].isna().sum()\n",
        "    desc['missing_pct'] = (desc['missing'] / max(len(df),1) * 100).round(2)\n",
        "    return desc\n",
        "\n",
        "def describe_categorical(df: pd.DataFrame, cols: Optional[List[str]] = None):\n",
        "    if cols is None:\n",
        "        cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    result = []\n",
        "    for col in cols:\n",
        "        vc = df[col].value_counts(dropna=True)\n",
        "        top = vc.index[0] if not vc.empty else None\n",
        "        top_freq = int(vc.iloc[0]) if not vc.empty else 0\n",
        "        result.append({\n",
        "            'column': col,\n",
        "            'unique': int(df[col].nunique(dropna=True)),\n",
        "            'top': top,\n",
        "            'top_freq': top_freq,\n",
        "            'missing': int(df[col].isna().sum()),\n",
        "            'missing_pct': float(df[col].isna().sum() / max(len(df),1) * 100)\n",
        "        })\n",
        "    return pd.DataFrame(result)\n",
        "\n",
        "print(\"âœ… Helper functions loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data-loading",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "Load synthetic or real data for EDA development and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sample_data(n_rows: int = 1000, seed: int = 42) -> pd.DataFrame:\n",
        "    np.random.seed(seed)\n",
        "    dates = pd.date_range('2023-01-01', periods=n_rows, freq='D')\n",
        "    df = pd.DataFrame({\n",
        "        'date': dates,\n",
        "        'sales': 100 + 0.3 * np.arange(n_rows) + 15 * np.sin(2 * np.pi * np.arange(n_rows) / 7) + np.random.normal(0, 5, n_rows),\n",
        "        'customers': np.random.poisson(50, n_rows),\n",
        "        'revenue': np.random.uniform(1000, 10000, n_rows),\n",
        "        'region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),\n",
        "        'product_category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], n_rows),\n",
        "        'satisfaction': np.random.uniform(1, 5, n_rows),\n",
        "        'discount': np.random.choice([0, 5, 10, 15, 20], n_rows, p=[0.4, 0.2, 0.2, 0.15, 0.05]),\n",
        "        'is_weekend': (dates.dayofweek >= 5).astype(int),\n",
        "        'temperature': 15 + 10 * np.sin(2 * np.pi * np.arange(n_rows) / 365) + np.random.normal(0, 3, n_rows),\n",
        "    })\n",
        "    df.loc[np.random.choice(df.index, size=int(0.05 * n_rows), replace=False), 'satisfaction'] = np.nan\n",
        "    df.loc[np.random.choice(df.index, size=int(0.02 * n_rows), replace=False), 'temperature'] = np.nan\n",
        "    return df\n",
        "\n",
        "df = generate_sample_data(n_rows=1000)\n",
        "display_info(df, \"Generated Sample Dataset\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data-quality",
      "metadata": {},
      "source": [
        "## 3. Data Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quality-check",
      "metadata": {},
      "outputs": [],
      "source": [
        "validation = validate_dataframe_for_ml(df, target='sales')\n",
        "print(\"ðŸ” Validation Results:\")\n",
        "print(f\"\\nValid for ML: {'âœ… Yes' if validation['is_valid'] else 'âŒ No'}\")\n",
        "if validation['errors']:\n",
        "    print(\"\\nâš ï¸ Errors:\")\n",
        "    for e in validation['errors']:\n",
        "        print(' -', e)\n",
        "if validation['warnings']:\n",
        "    print(\"\\nâš ï¸ Warnings:\")\n",
        "    for w in validation['warnings']:\n",
        "        print(' -', w)\n",
        "\n",
        "df_clean = clean_data(df.copy(), remove_duplicates=True, handle_missing='auto', detect_outliers=True)\n",
        "print(\"\\nðŸ§¹ Cleaning done. Rows:\", len(df), 'â†’', len(df_clean))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "statistical-analysis",
      "metadata": {},
      "source": [
        "## 4. Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numeric-stats",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“Š Numeric Statistics (Enhanced):\\n\")\n",
        "numeric_stats = describe_numeric(df_clean)\n",
        "numeric_stats.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "categorical-stats",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“Š Categorical Statistics:\\n\")\n",
        "cat_stats = describe_categorical(df_clean)\n",
        "cat_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "correlation",
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "correlation_matrix = df_clean[numeric_cols].corr()\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "corr_pairs = correlation_matrix.mask(mask).stack().reset_index()\n",
        "corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "corr_pairs = corr_pairs[abs(corr_pairs['Correlation']) > 0.3].sort_values('Correlation', ascending=False)\n",
        "corr_pairs.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "univariate",
      "metadata": {},
      "source": [
        "## 5. Univariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numeric-distributions",
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for col in numeric_cols[:4]:\n",
        "    fig = histogram(df_clean, col=col, binning_method='freedman_diaconis',\n",
        "                    show_mean=True, show_median=True, show_std=True, marginal='box',\n",
        "                    title=f\"Distribution of {col}\")\n",
        "    fig.show()\n",
        "    try:\n",
        "        _, p_value = stats.normaltest(df_clean[col].dropna())\n",
        "        print(f\"\\n{col}: Normality test p-value: {p_value:.4f} | \", 'âœ… Normal' if p_value>0.05 else 'âŒ Not normal')\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{col}: normality test skipped ({e})\")\n",
        "    print('-'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "categorical-distributions",
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    value_counts = df_clean[col].value_counts()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x=value_counts.index.astype(str), y=value_counts.values, text=value_counts.values, textposition='auto'))\n",
        "    fig.update_layout(title=f\"Distribution of {col}\", xaxis_title=col, yaxis_title='Count', template='plotly_white', height=400)\n",
        "    fig.show()\n",
        "    print(f\"\\n{col}: Unique values: {df_clean[col].nunique()} | Most common: {value_counts.index[0] if len(value_counts)>0 else 'N/A'}\")\n",
        "    print('-'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "box-plots",
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in numeric_cols[:4]:\n",
        "    fig = box(df_clean, y=col, title=f\"Box Plot: {col}\", points='outliers')\n",
        "    fig.show()\n",
        "    Q1, Q3 = df_clean[col].quantile(0.25), df_clean[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_clean[(df_clean[col] < Q1 - 1.5*IQR) | (df_clean[col] > Q3 + 1.5*IQR)]\n",
        "    print(f\"\\n{col}: Outliers: {len(outliers)} ({(len(outliers)/max(len(df_clean),1)*100):.2f}%)\")\n",
        "    print('-'*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bivariate",
      "metadata": {},
      "source": [
        "## 6. Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scatter-plots",
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = 'sales'\n",
        "for col in numeric_cols:\n",
        "    if col != target_col:\n",
        "        fig = scatter(df_clean, x=col, y=target_col, title=f\"{target_col} vs {col}\", trendline='ols')\n",
        "        fig.show()\n",
        "        corr = df_clean[[col, target_col]].corr().iloc[0,1]\n",
        "        print(f\"Correlation ({col} vs {target_col}): {corr:.4f}\")\n",
        "        print('-'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "categorical-relationships",
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in categorical_cols:\n",
        "    fig = box(df_clean, x=col, y=target_col, title=f\"{target_col} by {col}\", points='outliers')\n",
        "    fig.show()\n",
        "    groups = [df_clean[df_clean[col]==cat][target_col].dropna() for cat in df_clean[col].dropna().unique()]\n",
        "    if len(groups) >= 2 and all(len(g)>1 for g in groups):\n",
        "        f_stat, p_value = stats.f_oneway(*groups)\n",
        "        print(f\"ANOVA ({col} vs {target_col}): F={f_stat:.4f}, p={p_value:.4f} | \", 'âœ… Significant' if p_value<0.05 else 'âŒ Not significant')\n",
        "    else:\n",
        "        print(f\"ANOVA skipped for {col} (insufficient groups)\")\n",
        "    print('-'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "correlation-heatmap",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = correlation_heatmap(df_clean, method='pearson', annotate=True, cluster=True, cmap='RdBu_r', title='Correlation Matrix (Hierarchical)')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multivariate",
      "metadata": {},
      "source": [
        "## 7. Multivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pairplot",
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_subset = numeric_cols[:4]\n",
        "if _HAS_SNS and len(cols_subset)>=2:\n",
        "    g = sns.pairplot(df_clean[cols_subset], diag_kind='kde', plot_kws={'alpha':0.6}, height=2.5)\n",
        "    plt.suptitle('Pairwise Relationships', y=1.02, fontsize=14)\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Pairplot skipped (seaborn not installed or insufficient numeric columns).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scatter-3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(numeric_cols) >= 3:\n",
        "    fig = scatter_3d(df_clean, x=numeric_cols[0], y=numeric_cols[1], z=numeric_cols[2], color=categorical_cols[0] if categorical_cols else None,\n",
        "                     title=f\"3D Scatter: {numeric_cols[0]} vs {numeric_cols[1]} vs {numeric_cols[2]}\")\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "parallel-coordinates",
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_for_parallel = numeric_cols[:5]\n",
        "if len(cols_for_parallel)>=2:\n",
        "    color_col = categorical_cols[0] if categorical_cols else cols_for_parallel[0]\n",
        "    fig = px.parallel_coordinates(df_clean, dimensions=cols_for_parallel, color=df_clean[color_col] if color_col in df_clean.columns else None,\n",
        "                                  title='Parallel Coordinates Plot')\n",
        "    fig.show()\n",
        "else:\n",
        "    print('Parallel coordinates skipped (insufficient columns).')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced-viz",
      "metadata": {},
      "source": [
        "## 8. Advanced Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "time-series-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'date' in df_clean.columns:\n",
        "    fig = line(df_clean, x='date', y='sales', title='Sales Over Time', show_smoothing=True, smoothing_window=7)\n",
        "    fig.show()\n",
        "    df_clean['MA_7'] = df_clean['sales'].rolling(window=7).mean()\n",
        "    df_clean['MA_30'] = df_clean['sales'].rolling(window=30).mean()\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_trace(go.Scatter(x=df_clean['date'], y=df_clean['sales'], name='Sales', mode='lines', opacity=0.5))\n",
        "    fig2.add_trace(go.Scatter(x=df_clean['date'], y=df_clean['MA_7'], name='7-Day MA', mode='lines'))\n",
        "    fig2.add_trace(go.Scatter(x=df_clean['date'], y=df_clean['MA_30'], name='30-Day MA', mode='lines'))\n",
        "    fig2.update_layout(title='Sales with Moving Averages', xaxis_title='Date', yaxis_title='Sales', template='plotly_white', hovermode='x unified')\n",
        "    fig2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "violin-plots",
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(categorical_cols)>0 and len(numeric_cols)>0:\n",
        "    for num_col in numeric_cols[:2]:\n",
        "        fig = violin(df_clean, x=categorical_cols[0], y=num_col, box=True, points='outliers',\n",
        "                     title=f\"{num_col} Distribution by {categorical_cols[0]}\")\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunburst-chart",
      "metadata": {},
      "outputs": [],
      "source": [
        "agg_df = None\n",
        "if len(categorical_cols) >= 2:\n",
        "    agg_df = df_clean.groupby(categorical_cols[:2])['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
        "    fig = px.sunburst(agg_df, path=categorical_cols[:2], values='sum', title='Sales Distribution by Categories', color='mean', color_continuous_scale='RdYlGn')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "treemap",
      "metadata": {},
      "outputs": [],
      "source": [
        "if isinstance(agg_df, pd.DataFrame):\n",
        "    fig = px.treemap(agg_df, path=categorical_cols[:2], values='sum', color='mean', title='Sales Treemap', color_continuous_scale='Viridis')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "density-contour",
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(numeric_cols) >= 2:\n",
        "    fig = px.density_contour(df_clean, x=numeric_cols[0], y=numeric_cols[1], title=f\"Density Contour: {numeric_cols[0]} vs {numeric_cols[1]}\",\n",
        "                              marginal_x='histogram', marginal_y='histogram')\n",
        "    fig.update_traces(contours_coloring='fill', contours_showlabels=True)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "profiling",
      "metadata": {},
      "source": [
        "## 9. Automated Profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ydata-profiling",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    profile_html = generate_profile(df_clean, title='Comprehensive Data Profile', dark_mode=True, minimal=False)\n",
        "    output_path = Path('data/exports/eda_profile_report.html')\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    output_path.write_text(profile_html, encoding='utf-8')\n",
        "    print(f\"âœ… Profile report saved to: {output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not generate profile report: {e}\\nThis is optional - continuing...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dashboards",
      "metadata": {},
      "source": [
        "## 10. Dashboard Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "combined-dashboard",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
        "    'Sales Distribution', 'Sales Over Time', 'Sales by Region', 'Revenue vs Sales'\n",
        "), specs=[[{\"type\":\"histogram\"}, {\"type\":\"scatter\"}], [{\"type\":\"box\"}, {\"type\":\"scatter\"}]])\n",
        "fig.add_trace(go.Histogram(x=df_clean['sales'], name='Sales'), row=1, col=1)\n",
        "if 'date' in df_clean.columns:\n",
        "    fig.add_trace(go.Scatter(x=df_clean['date'], y=df_clean['sales'], mode='lines', name='Sales'), row=1, col=2)\n",
        "if len(categorical_cols)>0:\n",
        "    for category in df_clean[categorical_cols[0]].dropna().unique():\n",
        "        data = df_clean[df_clean[categorical_cols[0]] == category]['sales']\n",
        "        fig.add_trace(go.Box(y=data, name=str(category)), row=2, col=1)\n",
        "fig.add_trace(go.Scatter(x=df_clean['revenue'], y=df_clean['sales'], mode='markers', name='Revenue vs Sales'), row=2, col=2)\n",
        "fig.update_layout(height=800, showlegend=False, title_text='Comprehensive EDA Dashboard', template='plotly_white')\n",
        "fig.show()\n",
        "print('âœ… Combined dashboard created!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "export",
      "metadata": {},
      "source": [
        "## 11. Export & Reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export-figures",
      "metadata": {},
      "outputs": [],
      "source": [
        "export_dir = Path('data/exports/eda_notebook')\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "print('ðŸ“¤ Exporting visualizations...')\n",
        "hist_fig = histogram(df_clean, col='sales', binning_method='freedman_diaconis', title='Sales Distribution')\n",
        "hist_fig.write_html(str(export_dir / 'sales_histogram.html'))\n",
        "corr_fig = correlation_heatmap(df_clean, method='pearson', annotate=True, title='Correlation Matrix')\n",
        "corr_fig.write_html(str(export_dir / 'correlation_heatmap.html'))\n",
        "fig.write_html(str(export_dir / 'eda_dashboard.html'))\n",
        "print(f\"âœ… Saved to: {export_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clean.to_csv(export_dir / 'cleaned_data.csv', index=False)\n",
        "summary_stats = {\n",
        "    'numeric_stats': describe_numeric(df_clean).to_dict(),\n",
        "    'categorical_stats': describe_categorical(df_clean).to_dict(),\n",
        "    'correlation_matrix': correlation_matrix.to_dict()\n",
        "}\n",
        "with open(export_dir / 'summary_statistics.json', 'w') as f:\n",
        "    json.dump(summary_stats, f, indent=2)\n",
        "print('âœ… Saved: cleaned_data.csv, summary_statistics.json')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}