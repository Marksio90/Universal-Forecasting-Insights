{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5649485f",
      "metadata": {},
      "source": [
        "# ðŸ¤– Intelligent Predictor PRO++++ â€” Model Experiments Notebook\n",
        "\n",
        "**Purpose:** Development and experimentation environment for ML models, hyperparameter tuning, and model comparison\n",
        "\n",
        "**Author:** Data Science Team  \n",
        "**Date:** 2025-10-09  \n",
        "**Version:** 2.0.1\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Table of Contents\n",
        "\n",
        "1. [Setup & Imports](#setup)\n",
        "2. [Data Loading & Preprocessing](#data-loading)\n",
        "3. [Baseline Models](#baseline)\n",
        "4. [AutoML Pipeline](#automl)\n",
        "5. [Hyperparameter Tuning](#hpo)\n",
        "6. [Model Comparison](#comparison)\n",
        "7. [Ensemble Methods](#ensemble)\n",
        "8. [Feature Importance & SHAP](#shap)\n",
        "9. [Model Registry](#registry)\n",
        "10. [Time Series Forecasting](#timeseries)\n",
        "11. [Anomaly Detection](#anomaly)\n",
        "12. [Model Deployment](#deployment)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e715f645",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f583ebd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, time, warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score, confusion_matrix)\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception: lgb = None\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception: xgb = None\n",
        "try:\n",
        "    import catboost as cb\n",
        "    _HAS_CAT = True\n",
        "except Exception:\n",
        "    _HAS_CAT = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    _HAS_SHAP = True\n",
        "except Exception:\n",
        "    _HAS_SHAP = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Optional project modules (safe import)\n",
        "def _safe_import(mod, attr):\n",
        "    try:\n",
        "        m = __import__(mod, fromlist=[attr])\n",
        "        return getattr(m, attr)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "clean_data = _safe_import('src.data_processing.data_cleaner','clean_data')\n",
        "engineer_features = _safe_import('src.data_processing.feature_engineering','engineer_features')\n",
        "train_automl = _safe_import('src.ml_models.automl_pipeline','train_automl')\n",
        "detect_problem_type = _safe_import('src.ml_models.automl_pipeline','detect_problem_type')\n",
        "register_model = _safe_import('src.ml_models.model_registry','register_model')\n",
        "list_models = _safe_import('src.ml_models.model_registry','list_models')\n",
        "get_best_model = _safe_import('src.ml_models.model_registry','get_best_model')\n",
        "load_model = _safe_import('src.ml_models.model_registry','load_model')\n",
        "get_shap_values = _safe_import('src.ml_models.explainability','get_shap_values')\n",
        "plot_shap_summary = _safe_import('src.ml_models.explainability','plot_shap_summary')\n",
        "forecast = _safe_import('src.ml_models.forecasting','forecast')\n",
        "detect_anomalies = _safe_import('src.ml_models.anomaly_detection','detect_anomalies')\n",
        "\n",
        "# Fallbacks\n",
        "if clean_data is None:\n",
        "    def clean_data(df, remove_duplicates=True, handle_missing='auto'):\n",
        "        out = df.copy()\n",
        "        if remove_duplicates: out = out.drop_duplicates()\n",
        "        if handle_missing=='auto':\n",
        "            for c in out.select_dtypes(include=[np.number]).columns: out[c]=out[c].fillna(out[c].median())\n",
        "            for c in out.select_dtypes(include=['object','category']).columns:\n",
        "                mv = out[c].mode()\n",
        "                out[c]=out[c].fillna(mv.iloc[0] if not mv.empty else 'NA')\n",
        "        return out\n",
        "\n",
        "if engineer_features is None:\n",
        "    def engineer_features(df, **kwargs):\n",
        "        out = pd.get_dummies(df, columns=list(df.select_dtypes(include=['object','category']).columns), drop_first=True)\n",
        "        return out\n",
        "\n",
        "if detect_problem_type is None:\n",
        "    def detect_problem_type(df, target):\n",
        "        y = df[target]\n",
        "        return 'regression' if (y.dtype.kind in 'ifu' and y.nunique()>20) else 'classification'\n",
        "\n",
        "if train_automl is None:\n",
        "    def train_automl(df, target, test_size=0.2, random_state=42, **kwargs):\n",
        "        prob = detect_problem_type(df, target)\n",
        "        X = df.drop(columns=[target]); y = df[target]\n",
        "        Xtr,Xte,Ytr,Yte = train_test_split(X,y,test_size=test_size,random_state=random_state, stratify=y if prob=='classification' else None)\n",
        "        best=None; best_metrics={}; best_score=-np.inf if prob=='classification' else np.inf\n",
        "        cands=[]\n",
        "        if prob=='classification':\n",
        "            if lgb: cands.append(lgb.LGBMClassifier(random_state=random_state, verbose=-1))\n",
        "            if xgb: cands.append(xgb.XGBClassifier(random_state=random_state, eval_metric='logloss', verbosity=0))\n",
        "            cands.append(RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1))\n",
        "        else:\n",
        "            if lgb: cands.append(lgb.LGBMRegressor(random_state=random_state, verbose=-1))\n",
        "            if xgb: cands.append(xgb.XGBRegressor(random_state=random_state, verbosity=0))\n",
        "            cands.append(RandomForestRegressor(n_estimators=300, random_state=random_state, n_jobs=-1))\n",
        "        for m in cands:\n",
        "            m.fit(Xtr,Ytr); yp=m.predict(Xte)\n",
        "            if prob=='classification':\n",
        "                try: proba=m.predict_proba(Xte)\n",
        "                except Exception: proba=None\n",
        "                met={'accuracy':accuracy_score(Yte,yp),'precision':precision_score(Yte,yp,average='weighted',zero_division=0),\n",
        "                     'recall':recall_score(Yte,yp,average='weighted',zero_division=0),'f1':f1_score(Yte,yp,average='weighted',zero_division=0)}\n",
        "                score=met['f1']\n",
        "                if score>best_score: best, best_metrics, best_score = m, met, score\n",
        "            else:\n",
        "                met={'rmse':float(np.sqrt(mean_squared_error(Yte,yp))),'mae':float(mean_absolute_error(Yte,yp)),\n",
        "                     'r2':float(r2_score(Yte,yp))}\n",
        "                score=-met['rmse']\n",
        "                if score> -best_score: best, best_metrics, best_score = m, met, met['rmse']\n",
        "        return best, best_metrics, prob\n",
        "\n",
        "if get_shap_values is None or plot_shap_summary is None:\n",
        "    def get_shap_values(model, X):\n",
        "        if not _HAS_SHAP: raise RuntimeError(\"SHAP not installed\")\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(model); return explainer(X)\n",
        "        except Exception:\n",
        "            ke = shap.KernelExplainer(model.predict, X.sample(min(100,len(X)), random_state=42))\n",
        "            return ke.shap_values(X.sample(min(200,len(X)), random_state=42))\n",
        "    def plot_shap_summary(shap_values, X):\n",
        "        if hasattr(shap_values,'values'):\n",
        "            shap.plots.beeswarm(shap_values, max_display=20, show=False); return plt.gcf()\n",
        "        shap.summary_plot(shap_values, X, show=False, max_display=20); return plt.gcf()\n",
        "\n",
        "if forecast is None:\n",
        "    def forecast(df, target, horizon, freq='D', **kwargs):\n",
        "        try:\n",
        "            try:\n",
        "                from prophet import Prophet\n",
        "            except Exception:\n",
        "                from fbprophet import Prophet  # type: ignore\n",
        "            tmp = df.rename(columns={'date':'ds', target:'y'})[['ds','y']]\n",
        "            m = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)\n",
        "            m.fit(tmp); fut = m.make_future_dataframe(periods=horizon, freq=freq); fc=m.predict(fut)\n",
        "            return m, fc\n",
        "        except Exception:\n",
        "            series = df[target].astype(float).values; last=float(series[-1]) if len(series) else 0.0\n",
        "            dates = pd.date_range(df['date'].iloc[-1]+pd.Timedelta(1,unit=freq), periods=horizon, freq=freq)\n",
        "            std = float(np.std(series[-min(30,len(series)):] or np.array([1.0])))\n",
        "            fc = pd.DataFrame({'ds':dates,'yhat':np.full(horizon,last),'yhat_lower':last-1.96*std,'yhat_upper':last+1.96*std})\n",
        "            class N: pass\n",
        "            return N(), fc\n",
        "\n",
        "if detect_anomalies is None:\n",
        "    def detect_anomalies(df, method='isolation_forest', contamination=0.05):\n",
        "        from sklearn.ensemble import IsolationForest\n",
        "        from sklearn.neighbors import LocalOutlierFactor\n",
        "        X = df.select_dtypes(include=[np.number])\n",
        "        out = df.copy()\n",
        "        if method=='isolation_forest':\n",
        "            iso = IsolationForest(contamination=contamination, random_state=42)\n",
        "            out['is_anomaly'] = (iso.fit_predict(X)==-1).astype(int)\n",
        "        elif method=='lof':\n",
        "            lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n",
        "            out['is_anomaly'] = (lof.fit_predict(X)==-1).astype(int)\n",
        "        else:\n",
        "            z = np.abs(stats.zscore(X, nan_policy='omit'))\n",
        "            out['is_anomaly'] = ((z>3).any(axis=1)).astype(int)\n",
        "        return out\n",
        "\n",
        "# Simple registry fallback\n",
        "_REGISTRY = Path('models/registry.json')\n",
        "if register_model is None:\n",
        "    def register_model(model_path, target, problem_type, metrics, tags, extra):\n",
        "        _REGISTRY.parent.mkdir(parents=True, exist_ok=True)\n",
        "        reg = (json.load(open(_REGISTRY)) if _REGISTRY.exists() else [])\n",
        "        entry = {'id': f\"mdl_{int(time.time())}\", 'path': model_path, 'target': target, 'problem_type': problem_type,\n",
        "                 'algorithm': Path(model_path).stem, 'timestamp': datetime.now().isoformat(), 'metrics': metrics,\n",
        "                 'tags': tags, 'extra': extra}\n",
        "        reg.append(entry); json.dump(reg, open(_REGISTRY,'w'), indent=2); return entry\n",
        "if list_models is None:\n",
        "    def list_models(): return (json.load(open(_REGISTRY)) if _REGISTRY.exists() else [])\n",
        "if get_best_model is None:\n",
        "    def get_best_model(problem_type, metric='f1'):\n",
        "        models = [m for m in list_models() if m['problem_type']==problem_type]\n",
        "        if not models: return None\n",
        "        reverse = (metric!='rmse')\n",
        "        models.sort(key=lambda m: m['metrics'].get(metric, -np.inf if reverse else np.inf), reverse=reverse)\n",
        "        return models[0]\n",
        "if load_model is None:\n",
        "    def load_model(model_id):\n",
        "        import joblib\n",
        "        for m in list_models():\n",
        "            if m['id']==model_id: return joblib.load(m['path'])\n",
        "        return None\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "print(\"âœ… Imports OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a447cba7",
      "metadata": {},
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc03125",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_classification(y_true, y_pred, y_pred_proba=None):\n",
        "    metrics = {'accuracy': accuracy_score(y_true, y_pred),\n",
        "               'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "               'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "               'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0)}\n",
        "    if y_pred_proba is not None:\n",
        "        try: metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "        except Exception: metrics['roc_auc'] = None\n",
        "    return metrics\n",
        "\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    return {'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "            'mae': float(mean_absolute_error(y_true, y_pred)),\n",
        "            'r2': float(r2_score(y_true, y_pred))}\n",
        "\n",
        "def print_metrics(metrics: Dict[str, float], title: str = \"Model Metrics\"):\n",
        "    print(\"\\n\" + \"=\"*60); print(f\"ðŸ“Š {title}\"); print(\"=\"*60)\n",
        "    for k,v in metrics.items(): print(f\"{k.upper():12s}: {v:.4f}\" if v is not None else f\"{k.upper():12s}: N/A\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "def compare_models(results: List[Dict[str, Any]], metric: str='accuracy'):\n",
        "    df = pd.DataFrame(results).sort_values(metric, ascending=(metric=='rmse'))\n",
        "    display(df)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x=df['model_name'], y=df[metric], text=df[metric].round(4), textposition='auto'))\n",
        "    fig.update_layout(title=f\"Model Comparison: {metric.upper()}\", template=\"plotly_white\")\n",
        "    fig.show()\n",
        "    return df\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig = px.imshow(cm, labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"), x=labels, y=labels, text_auto=True)\n",
        "    fig.update_layout(title=\"Confusion Matrix\", template=\"plotly_white\"); fig.show()\n",
        "\n",
        "print(\"âœ… Helper functions loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614bb94b",
      "metadata": {},
      "source": [
        "## 2. Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c70b337",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_classification_data(n_samples=1000, n_features=12, n_classes=2, random_state=42):\n",
        "    from sklearn.datasets import make_classification\n",
        "    X,y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=int(n_features*0.7),\n",
        "                              n_redundant=int(n_features*0.2), n_classes=n_classes, random_state=random_state, flip_y=0.05)\n",
        "    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)]); df['target']=y\n",
        "    df['category_1'] = np.random.choice(['A','B','C'], n_samples); df['category_2'] = np.random.choice(['X','Y'], n_samples)\n",
        "    return df\n",
        "\n",
        "def generate_regression_data(n_samples=1000, n_features=12, noise=10, random_state=42):\n",
        "    from sklearn.datasets import make_regression\n",
        "    X,y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=int(n_features*0.7),\n",
        "                          noise=noise, random_state=random_state)\n",
        "    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)]); df['target']=y\n",
        "    df['category_1'] = np.random.choice(['A','B','C'], n_samples); df['category_2'] = np.random.choice(['X','Y'], n_samples)\n",
        "    return df\n",
        "\n",
        "TASK_TYPE = 'classification'  # or 'regression'\n",
        "df = generate_classification_data() if TASK_TYPE=='classification' else generate_regression_data()\n",
        "print(\"ðŸ“Š Dataset generated:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531ae50e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ§¹ Preprocessing...\")\n",
        "df_clean = clean_data(df)\n",
        "df_engineered = engineer_features(df_clean)\n",
        "print(\"âœ… Preprocessed:\", df_engineered.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc1ad51",
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET='target'; TEST_SIZE=0.2; VALIDATION_SIZE=0.1\n",
        "X = df_engineered.drop(columns=[TARGET]); y = df_engineered[TARGET]\n",
        "X_train_full, X_temp, y_train_full, y_temp = train_test_split(X,y, test_size=TEST_SIZE+VALIDATION_SIZE, random_state=42,\n",
        "                                                              stratify=y if TASK_TYPE=='classification' else None)\n",
        "val_ratio = VALIDATION_SIZE/(TEST_SIZE+VALIDATION_SIZE)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp,y_temp, test_size=(1-val_ratio), random_state=42,\n",
        "                                                stratify=y_temp if TASK_TYPE=='classification' else None)\n",
        "print(f\"Train/Val/Test: {len(X_train_full)}/{len(X_val)}/{len(X_test)} | Features: {X_train_full.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2557a695",
      "metadata": {},
      "source": [
        "## 3. Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a322630d",
      "metadata": {},
      "outputs": [],
      "source": [
        "results=[]\n",
        "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
        "\n",
        "if TASK_TYPE=='classification':\n",
        "    base=DummyClassifier(strategy='most_frequent')\n",
        "else:\n",
        "    base=DummyRegressor(strategy='mean')\n",
        "\n",
        "base.fit(X_train_full, y_train_full); yb=base.predict(X_test)\n",
        "\n",
        "met = evaluate_classification(y_test,yb) if TASK_TYPE=='classification' else evaluate_regression(y_test,yb)\n",
        "\n",
        "met['model_name']='Baseline'; met['training_time']=0.0; results.append(met)\n",
        "print_metrics(met, 'Baseline')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43f4f1d",
      "metadata": {},
      "source": [
        "## 4. AutoML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a417d2a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "ptype = detect_problem_type(df_engineered, TARGET); print(\"ðŸ“Š Problem type:\", ptype)\n",
        "st=time.time(); model, metrics, _ = train_automl(df_engineered, target=TARGET, test_size=TEST_SIZE, random_state=42); tt=time.time()-st\n",
        "metrics['model_name']=f\"AutoML ({type(model).__name__})\"; metrics['training_time']=tt; results.append(metrics)\n",
        "print_metrics(metrics, 'AutoML Best Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167b5530",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "if TASK_TYPE=='classification':\n",
        "    plot_confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
        "else:\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=y_test, y=y_pred, mode='markers', name='Predictions', opacity=0.6))\n",
        "    mn, mx = float(np.min(y_test)), float(np.max(y_test))\n",
        "    fig.add_trace(go.Scatter(x=[mn,mx], y=[mn,mx], mode='lines', name='Perfect', line=dict(dash='dash')))\n",
        "    fig.update_layout(title='Actual vs Predicted', template='plotly_white'); fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddfba8f3",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7e33cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "if TASK_TYPE=='classification':\n",
        "    base = lgb.LGBMClassifier(random_state=42, verbose=-1) if lgb else RandomForestClassifier(n_estimators=300, random_state=42)\n",
        "    scoring='f1_weighted'\n",
        "else:\n",
        "    base = lgb.LGBMRegressor(random_state=42, verbose=-1) if lgb else RandomForestRegressor(n_estimators=300, random_state=42)\n",
        "    scoring='neg_root_mean_squared_error'\n",
        "param_dist={'n_estimators':[100,200,300,500]}\n",
        "rs = RandomizedSearchCV(base, param_distributions=param_dist, n_iter=6, cv=3, scoring=scoring, random_state=42, n_jobs=-1, verbose=1)\n",
        "st=time.time(); rs.fit(X_train_full, y_train_full); tt=time.time()-st\n",
        "best = rs.best_estimator_; yp = best.predict(X_test)\n",
        "met = evaluate_classification(y_test, yp) if TASK_TYPE=='classification' else evaluate_regression(y_test, yp)\n",
        "met['model_name']=f'{type(best).__name__} (Tuned)'; met['training_time']=tt; results.append(met)\n",
        "print_metrics(met, 'Tuned Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333fc440",
      "metadata": {},
      "source": [
        "## 6. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f65c3d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "models_to_compare=[]\n",
        "if TASK_TYPE=='classification':\n",
        "    if lgb: models_to_compare.append(('LightGBM', lgb.LGBMClassifier(random_state=42, verbose=-1)))\n",
        "    if xgb: models_to_compare.append(('XGBoost', xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0)))\n",
        "    models_to_compare.append(('Random Forest', RandomForestClassifier(n_estimators=300, random_state=42)))\n",
        "else:\n",
        "    if lgb: models_to_compare.append(('LightGBM', lgb.LGBMRegressor(random_state=42, verbose=-1)))\n",
        "    if xgb: models_to_compare.append(('XGBoost', xgb.XGBRegressor(random_state=42, verbosity=0)))\n",
        "    models_to_compare.append(('Random Forest', RandomForestRegressor(n_estimators=300, random_state=42)))\n",
        "comparison_results=[]; trained_models_dict={}\n",
        "for name,mdl in models_to_compare:\n",
        "    st=time.time(); mdl.fit(X_train_full,y_train_full); tt=time.time()-st; yp=mdl.predict(X_test)\n",
        "    met = evaluate_classification(y_test, yp) if TASK_TYPE=='classification' else evaluate_regression(y_test, yp)\n",
        "    met['model_name']=name; met['training_time']=tt; comparison_results.append(met); trained_models_dict[name]=mdl\n",
        "comparison_df = compare_models(comparison_results, metric=('f1' if TASK_TYPE=='classification' else 'rmse'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cde93e6",
      "metadata": {},
      "source": [
        "## 7. Ensemble Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf08ea13",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "base_models = models_to_compare[:2] if len(models_to_compare)>=2 else models_to_compare\n",
        "if TASK_TYPE=='classification':\n",
        "    ens = VotingClassifier(estimators=base_models, voting='soft'); stk = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(max_iter=1000))\n",
        "else:\n",
        "    ens = VotingRegressor(estimators=base_models); stk = StackingRegressor(estimators=base_models, final_estimator=Ridge())\n",
        "ens.fit(X_train_full,y_train_full); stk.fit(X_train_full,y_train_full)\n",
        "yp_e = ens.predict(X_test); yp_s = stk.predict(X_test)\n",
        "met_e = evaluate_classification(y_test, yp_e) if TASK_TYPE=='classification' else evaluate_regression(y_test, yp_e)\n",
        "met_s = evaluate_classification(y_test, yp_s) if TASK_TYPE=='classification' else evaluate_regression(y_test, yp_s)\n",
        "met_e['model_name']='Voting Ensemble'; met_s['model_name']='Stacking Ensemble'\n",
        "final_df = compare_models(comparison_results+[met_e, met_s], metric=('f1' if TASK_TYPE=='classification' else 'rmse'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "510aaa7e",
      "metadata": {},
      "source": [
        "## 8. Feature Importance & SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3536771c",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_result = final_df.iloc[0]; best_model_name = best_model_result['model_name']\n",
        "best_model = trained_models_dict.get(best_model_name, model)\n",
        "if hasattr(best_model,'feature_importances_'):\n",
        "    imp = pd.DataFrame({'feature': X_train_full.columns, 'importance': best_model.feature_importances_}).sort_values('importance', ascending=False)\n",
        "    display(imp.head(10))\n",
        "if _HAS_SHAP:\n",
        "    try:\n",
        "        X_shap = X_test.sample(n=min(300,len(X_test)), random_state=42)\n",
        "        sv = get_shap_values(best_model, X_shap)\n",
        "        fig = plot_shap_summary(sv, X_shap); plt.show()\n",
        "    except Exception as e:\n",
        "        print(\"SHAP failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a42cbdf",
      "metadata": {},
      "source": [
        "## 9. Model Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0901df5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "models_dir = Path('models/trained_models'); models_dir.mkdir(parents=True, exist_ok=True)\n",
        "model_path = models_dir / f\"{best_model_name.replace(' ','_').lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
        "joblib.dump(best_model, model_path)\n",
        "entry = register_model(str(model_path), TARGET, TASK_TYPE, dict(best_model_result), ['best_model','experiment'], {'features': list(X_train_full.columns)})\n",
        "print(\"Registered:\", entry['id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd7e51b9",
      "metadata": {},
      "source": [
        "## 10. Time Series Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cbc40f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_timeseries_data(n_days=365, seed=42):\n",
        "    np.random.seed(seed); dates=pd.date_range('2023-01-01', periods=n_days, freq='D')\n",
        "    trend=100+0.3*np.arange(n_days); weekly=15*np.sin(2*np.pi*np.arange(n_days)/7); noise=np.random.normal(0,5,n_days)\n",
        "    return pd.DataFrame({'date':dates,'sales':trend+weekly+noise})\n",
        "df_ts = generate_timeseries_data(500)\n",
        "tr = df_ts.iloc[:400]; te = df_ts.iloc[400:]\n",
        "m, fc = forecast(tr, target='sales', horizon=len(te), freq='D')\n",
        "yhat = fc['yhat'].tail(len(te)).values; ytrue = te['sales'].values\n",
        "rmse = float(np.sqrt(mean_squared_error(ytrue, yhat))); mae=float(mean_absolute_error(ytrue, yhat))\n",
        "print(\"TS Metrics -> RMSE:\", rmse, \" MAE:\", mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4be27c",
      "metadata": {},
      "source": [
        "## 11. Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea77855",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_an = df_engineered.drop(columns=[TARGET])\n",
        "methods = ['isolation_forest','lof','statistical']; comp=[]\n",
        "for mth in methods:\n",
        "    tmp = detect_anomalies(df_an, method=mth, contamination=0.05)\n",
        "    comp.append({'Method': mth.upper(), 'Anomalies': int(tmp['is_anomaly'].sum())})\n",
        "disp = pd.DataFrame(comp); display(disp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6eea01",
      "metadata": {},
      "source": [
        "## 12. Model Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c616c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "import joblib, zipfile\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('model', best_model)])\n",
        "pipe.fit(X_train_full, y_train_full)\n",
        "models_dir = Path('models/trained_models'); models_dir.mkdir(parents=True, exist_ok=True)\n",
        "pipe_path = models_dir / \"production_pipeline.joblib\"; joblib.dump(pipe, pipe_path)\n",
        "# Package\n",
        "export_dir = Path('data/exports/deployment'); export_dir.mkdir(parents=True, exist_ok=True)\n",
        "zip_path = export_dir / f\"model_deployment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "    z.write(pipe_path, arcname='pipeline.joblib')\n",
        "    meta = {'model_name': best_model_name, 'created': datetime.now().isoformat(), 'features': list(X_train_full.columns)}\n",
        "    meta_path = export_dir / \"metadata.json\"; json.dump(meta, open(meta_path,'w'), indent=2); z.write(meta_path, arcname='metadata.json')\n",
        "print(\"Deployment package:\", zip_path.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd2604e9",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Experiment Summary\n",
        "\n",
        "End-to-end ML experimentation: baseline â†’ AutoML â†’ HPO â†’ comparison â†’ ensembles â†’ SHAP â†’ TS â†’ anomaly â†’ deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2035426",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70); print(\"ðŸŽ‰ MODEL EXPERIMENTS NOTEBOOK COMPLETE!\"); print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
