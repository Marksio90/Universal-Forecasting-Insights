"""
ModuÅ‚ AI Insights PRO - Zaawansowana analiza danych z wykorzystaniem AI.

FunkcjonalnoÅ›ci:
- Generowanie insightÃ³w z OpenAI/Claude
- Inteligentne budowanie promptÃ³w
- Retry logic z exponential backoff
- Streaming responses dla UX
- Historia wygenerowanych insightÃ³w
- Multi-format export (JSON/MD/TXT)
- Cache z bezpiecznym hashowaniem
"""

from __future__ import annotations

import io
import time
import json
import hashlib
import logging
from typing import Optional, Literal
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

import streamlit as st
import pandas as pd

from src.ai_engine.insights_generator import generate_insights
from src.utils.validators import basic_quality_checks

# ========================================================================================
# KONFIGURACJA
# ========================================================================================

logger = logging.getLogger(__name__)

# Limity bezpieczeÅ„stwa
MAX_PROMPT_TOKENS = 8000  # ~6000 sÅ‚Ã³w
MAX_SCHEMA_COLS = 50
MAX_RETRIES = 3
RETRY_DELAY = 2  # sekundy

# DomyÅ›lna struktura odpowiedzi
DEFAULT_RESPONSE_KEYS = ["summary", "top_insights", "recommendations", "risks"]


# ========================================================================================
# ENUMS & DATACLASSES
# ========================================================================================

class AnalysisDepth(str, Enum):
    """Tryb gÅ‚Ä™bokoÅ›ci analizy."""
    QUICK = "quick"
    DEEP = "deep"


@dataclass
class InsightsConfig:
    """Konfiguracja generowania insightÃ³w."""
    depth: AnalysisDepth
    include_schema: bool
    include_quality: bool
    show_json: bool


@dataclass
class DataContext:
    """Kontekst danych dla AI."""
    rows: int
    cols: int
    missing_pct: float
    dupes: int
    schema: list[dict]
    goal: str
    timestamp: str = ""
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()


@dataclass
class AIInsight:
    """Pojedynczy wygenerowany insight."""
    summary: str
    top_insights: list[str]
    recommendations: list[str]
    risks: list[str]
    metadata: dict
    timestamp: str
    
    def to_dict(self) -> dict:
        """Konwertuje do sÅ‚ownika."""
        return asdict(self)
    
    def to_json(self) -> str:
        """Konwertuje do JSON string."""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)
    
    def to_markdown(self) -> str:
        """Konwertuje do Markdown."""
        lines = [
            f"# AI Insights Report",
            f"\n**Generated:** {self.timestamp}\n",
            f"## ğŸ§­ Executive Summary\n",
            self.summary,
            f"\n## ğŸ“Œ Top Insights\n"
        ]
        
        for i, insight in enumerate(self.top_insights, 1):
            lines.append(f"{i}. {insight}")
        
        lines.append(f"\n## ğŸ’¡ Recommendations\n")
        for rec in self.recommendations:
            lines.append(f"- {rec}")
        
        lines.append(f"\n## âš ï¸ Risks & Anomalies\n")
        for risk in self.risks:
            lines.append(f"- {risk}")
        
        return "\n".join(lines)


# ========================================================================================
# UTILITY FUNCTIONS
# ========================================================================================

def _validate_dataframe(df: Optional[pd.DataFrame]) -> pd.DataFrame:
    """
    Waliduje DataFrame z session state.
    
    Args:
        df: DataFrame do walidacji
        
    Returns:
        Zwalidowany DataFrame
        
    Raises:
        ValueError: JeÅ›li DataFrame jest nieprawidÅ‚owy
    """
    if df is None:
        raise ValueError("Brak danych w session state")
    
    if not isinstance(df, pd.DataFrame):
        raise ValueError(f"Oczekiwano DataFrame, otrzymano {type(df)}")
    
    if df.empty:
        raise ValueError("DataFrame jest pusty")
    
    return df


def _extract_data_context(df: pd.DataFrame, goal: str) -> DataContext:
    """
    Ekstrahuje kontekst danych dla AI.
    
    Args:
        df: DataFrame do analizy
        goal: Cel biznesowy
        
    Returns:
        DataContext z metadanymi
    """
    # Statystyki jakoÅ›ci
    stats = basic_quality_checks(df)
    
    # Schema (ograniczony do MAX_SCHEMA_COLS)
    schema = []
    for col in df.columns[:MAX_SCHEMA_COLS]:
        try:
            nunique = int(df[col].nunique(dropna=True))
            dtype = str(df[col].dtype)
            
            schema.append({
                "col": col,
                "dtype": dtype,
                "nunique": nunique,
                "missing": float(df[col].isna().mean())
            })
        except Exception as e:
            logger.warning(f"BÅ‚Ä…d przetwarzania kolumny {col}: {e}")
            continue
    
    return DataContext(
        rows=stats["rows"],
        cols=stats["cols"],
        missing_pct=round(stats["missing_pct"] * 100, 2),
        dupes=stats["dupes"],
        schema=schema,
        goal=goal or "Nie podano celu biznesowego"
    )


def _build_prompt(context: DataContext, config: InsightsConfig) -> str:
    """
    Buduje prompt dla AI z walidacjÄ… rozmiaru.
    
    Args:
        context: Kontekst danych
        config: Konfiguracja analizy
        
    Returns:
        Zbudowany prompt
    """
    parts = []
    
    # Cel
    parts.append(f"**Cel analizy:** {context.goal}")
    
    # Podstawowe metryki
    parts.append(
        f"**Rozmiar danych:** {context.rows:,} wierszy Ã— {context.cols} kolumn\n"
        f"**Braki:** {context.missing_pct}%\n"
        f"**Duplikaty:** {context.dupes:,}"
    )
    
    # Schema
    if config.include_schema and context.schema:
        parts.append("\n**Struktura danych:**")
        schema_lines = []
        for s in context.schema[:25]:  # Limit dla promptu
            schema_lines.append(
                f"- `{s['col']}` ({s['dtype']}, "
                f"{s['nunique']} unikalnych, "
                f"{s['missing']*100:.1f}% brakÃ³w)"
            )
        
        if len(context.schema) > 25:
            schema_lines.append(f"... i {len(context.schema) - 25} innych kolumn")
        
        parts.append("\n".join(schema_lines))
    
    # Metryki jakoÅ›ci
    if config.include_quality:
        quality_metrics = {
            "rows": context.rows,
            "cols": context.cols,
            "missing_pct": context.missing_pct,
            "dupes": context.dupes
        }
        parts.append(
            "\n**Metryki jakoÅ›ci:**\n```json\n" +
            json.dumps(quality_metrics, indent=2) +
            "\n```"
        )
    
    # Instrukcja dla AI
    parts.append(
        "\n**Zadanie:**\n"
        "Wygeneruj syntetyczny raport analityczny w formacie JSON z kluczami:\n"
        "- `summary`: zwiÄ™zÅ‚e podsumowanie (2-3 zdania)\n"
        "- `top_insights`: lista 5-7 kluczowych obserwacji\n"
        "- `recommendations`: lista 3-5 rekomendacji akcji\n"
        "- `risks`: lista 2-4 potencjalnych ryzyk/anomalii"
    )
    
    # Dodatkowe instrukcje dla trybu gÅ‚Ä™bokiego
    if config.depth == AnalysisDepth.DEEP:
        parts.append(
            "\n**Tryb pogÅ‚Ä™biony:**\n"
            "- Dodaj wiÄ™cej szczegÃ³Å‚Ã³w i kontekstu\n"
            "- Zaproponuj konkretne KPI do monitorowania\n"
            "- Przedstaw hipotezy do testowania\n"
            "- UwzglÄ™dnij perspektywÄ™ biznesowÄ…"
        )
    
    prompt = "\n\n".join(parts)
    
    # Walidacja rozmiaru (przybliÅ¼ona)
    estimated_tokens = len(prompt.split())
    if estimated_tokens > MAX_PROMPT_TOKENS:
        logger.warning(
            f"Prompt przekracza limit ({estimated_tokens} > {MAX_PROMPT_TOKENS})"
        )
        # SkrÃ³Ä‡ schema
        if config.include_schema:
            logger.info("Skracam schema w prompcie")
            # Rebuild z mniejszym schema
            config.include_schema = False
            return _build_prompt(context, config)
    
    return prompt


def _create_cache_key(df: pd.DataFrame, prompt: str, config: InsightsConfig) -> str:
    """
    Tworzy bezpieczny klucz cache dla insightÃ³w.
    
    Args:
        df: DataFrame
        prompt: Prompt dla AI
        config: Konfiguracja
        
    Returns:
        SHA-256 hash jako klucz
    """
    # Komponenty klucza
    components = [
        str(len(df)),
        str(df.shape[1]),
        ",".join(sorted(df.columns)),
        prompt,
        config.depth.value,
        str(config.include_schema),
        str(config.include_quality)
    ]
    
    # Secure hash
    key_str = "|".join(components)
    return hashlib.sha256(key_str.encode()).hexdigest()


def _parse_ai_response(response: str) -> dict:
    """
    Parsuje odpowiedÅº AI do struktury JSON.
    
    Args:
        response: Surowa odpowiedÅº z AI
        
    Returns:
        SÅ‚ownik z sparsowanymi danymi
    """
    # PrÃ³ba bezpoÅ›redniego parsowania JSON
    try:
        data = json.loads(response)
        if isinstance(data, dict):
            return data
    except json.JSONDecodeError:
        pass
    
    # PrÃ³ba wyekstrahowania JSON z markdown code block
    try:
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            json_str = response[json_start:json_end].strip()
            data = json.loads(json_str)
            if isinstance(data, dict):
                return data
    except Exception:
        pass
    
    # Fallback: caÅ‚oÅ›Ä‡ jako summary
    return {
        "summary": response,
        "top_insights": [],
        "recommendations": [],
        "risks": []
    }


def _normalize_insights(data: dict) -> AIInsight:
    """
    Normalizuje dane z AI do struktury AIInsight.
    
    Args:
        data: Surowe dane z AI
        
    Returns:
        Znormalizowany AIInsight
    """
    # WyciÄ…gnij wartoÅ›ci z fallbackami
    summary = data.get("summary", "Brak podsumowania")
    
    # Ensure lists
    top_insights = data.get("top_insights", [])
    if isinstance(top_insights, str):
        top_insights = [top_insights]
    
    recommendations = data.get("recommendations", [])
    if isinstance(recommendations, str):
        recommendations = [recommendations]
    
    risks = data.get("risks", [])
    if isinstance(risks, str):
        risks = [risks]
    
    # Metadata
    metadata = {
        "response_keys": list(data.keys()),
        "processed_at": datetime.now().isoformat()
    }
    
    return AIInsight(
        summary=summary,
        top_insights=top_insights,
        recommendations=recommendations,
        risks=risks,
        metadata=metadata,
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )


@st.cache_data(show_spinner=False, ttl=3600)
def _cached_generate_insights(
    cache_key: str,
    df_sample: pd.DataFrame,
    prompt: str
) -> dict:
    """
    Cachowane generowanie insightÃ³w z retry logic.
    
    Args:
        cache_key: Klucz cache (dla unikalnoÅ›ci)
        df_sample: PrÃ³bka DataFrame
        prompt: Prompt dla AI
        
    Returns:
        SÅ‚ownik z wygenerowanymi insightami
    """
    for attempt in range(MAX_RETRIES):
        try:
            logger.info(f"Generowanie insightÃ³w (prÃ³ba {attempt + 1}/{MAX_RETRIES})")
            
            # WywoÅ‚anie AI
            response = generate_insights(df_sample, prompt)
            
            # Parsowanie
            parsed = _parse_ai_response(response)
            
            return parsed
            
        except Exception as e:
            logger.error(f"BÅ‚Ä…d generowania insightÃ³w (prÃ³ba {attempt + 1}): {e}")
            
            if attempt < MAX_RETRIES - 1:
                wait_time = RETRY_DELAY * (2 ** attempt)  # Exponential backoff
                logger.info(f"Retry za {wait_time}s...")
                time.sleep(wait_time)
            else:
                # Ostatnia prÃ³ba failed - zwrÃ³Ä‡ error
                return {
                    "summary": f"âŒ BÅ‚Ä…d generowania insightÃ³w po {MAX_RETRIES} prÃ³bach: {str(e)}",
                    "top_insights": [],
                    "recommendations": ["SprawdÅº poÅ‚Ä…czenie z API", "SprÃ³buj ponownie za chwilÄ™"],
                    "risks": ["Brak dostÄ™pu do AI insights"]
                }
    
    # Fallback (nie powinno siÄ™ zdarzyÄ‡)
    return {
        "summary": "Brak wynikÃ³w",
        "top_insights": [],
        "recommendations": [],
        "risks": []
    }


def _add_to_history(insight: AIInsight) -> None:
    """
    Dodaje insight do historii w session state.
    
    Args:
        insight: AIInsight do zapisania
    """
    if "insights_history" not in st.session_state:
        st.session_state["insights_history"] = []
    
    # Dodaj na poczÄ…tek (newest first)
    st.session_state["insights_history"].insert(0, insight.to_dict())
    
    # Ogranicz historiÄ™ do 10 ostatnich
    st.session_state["insights_history"] = st.session_state["insights_history"][:10]


# ========================================================================================
# STREAMLIT UI
# ========================================================================================

st.title("ğŸ¤– AI Insights â€” PRO")

# ========================================================================================
# WALIDACJA DANYCH
# ========================================================================================

try:
    df_raw = st.session_state.get("df") or st.session_state.get("df_raw")
    df_main = _validate_dataframe(df_raw)
except ValueError as e:
    st.warning(f"âš ï¸ {e}")
    st.info("PrzejdÅº do **ğŸ“¤ Upload Data**, aby wczytaÄ‡ dane.")
    st.stop()

# Cel biznesowy
goal = st.session_state.get("goal", "")

# ========================================================================================
# SIDEBAR: KONFIGURACJA
# ========================================================================================

with st.sidebar:
    st.subheader("âš™ï¸ Ustawienia analizy AI")
    
    depth_option = st.radio(
        "Tryb analizy",
        options=[
            "ğŸš€ Szybka (krÃ³tkie wnioski)",
            "ğŸ”¬ PogÅ‚Ä™biona (peÅ‚ny raport)"
        ],
        index=1,
        help="Szybka: ~30s, PogÅ‚Ä™biona: ~60s"
    )
    
    depth = AnalysisDepth.DEEP if "PogÅ‚Ä™biona" in depth_option else AnalysisDepth.QUICK
    
    include_schema = st.checkbox(
        "ğŸ“‹ UwzglÄ™dnij strukturÄ™ kolumn",
        value=True,
        help="WysyÅ‚a schema danych do AI"
    )
    
    include_quality = st.checkbox(
        "ğŸ“Š UwzglÄ™dnij metryki jakoÅ›ci",
        value=True,
        help="Dodaje statystyki brakÃ³w, duplikatÃ³w itp."
    )
    
    st.divider()
    
    show_json = st.checkbox(
        "ğŸ” PokaÅ¼ surowy JSON",
        value=False,
        help="Debug: wyÅ›wietl surowÄ… odpowiedÅº AI"
    )
    
    st.divider()
    
    # Historia
    history_count = len(st.session_state.get("insights_history", []))
    st.caption(f"ğŸ“š Historia: {history_count} raportÃ³w")

# Konfiguracja
config = InsightsConfig(
    depth=depth,
    include_schema=include_schema,
    include_quality=include_quality,
    show_json=show_json
)

# ========================================================================================
# KONTEKST I PROMPT
# ========================================================================================

st.subheader("ğŸ“ Kontekst analizy")

# WyÅ›wietl cel
if goal:
    st.info(f"**ğŸ¯ Cel:** {goal}")
else:
    st.warning("âš ï¸ Brak celu biznesowego. UzupeÅ‚nij w zakÅ‚adce Upload.")

# Ekstrahuj kontekst
with st.spinner("ğŸ“Š PrzygotowujÄ™ kontekst danych..."):
    try:
        context = _extract_data_context(df_main, goal)
        
        # PokaÅ¼ podstawowe info
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Wiersze", f"{context.rows:,}")
        col2.metric("Kolumny", f"{context.cols}")
        col3.metric("Braki", f"{context.missing_pct}%")
        col4.metric("Duplikaty", f"{context.dupes:,}")
        
    except Exception as e:
        st.error(f"âŒ BÅ‚Ä…d przygotowania kontekstu: {e}")
        logger.error(f"BÅ‚Ä…d kontekstu: {e}", exc_info=True)
        st.stop()

# Zbuduj prompt
try:
    prompt = _build_prompt(context, config)
    
    with st.expander("ğŸ‘ï¸ PodglÄ…d promptu dla AI", expanded=False):
        st.text_area(
            "Prompt",
            prompt,
            height=300,
            disabled=True,
            label_visibility="collapsed"
        )
        st.caption(f"Szacunkowa dÅ‚ugoÅ›Ä‡: ~{len(prompt.split())} sÅ‚Ã³w")
        
except Exception as e:
    st.error(f"âŒ BÅ‚Ä…d budowania promptu: {e}")
    logger.error(f"BÅ‚Ä…d promptu: {e}", exc_info=True)
    st.stop()

# ========================================================================================
# GENEROWANIE INSIGHTÃ“W
# ========================================================================================

st.divider()

generate_col1, generate_col2 = st.columns([3, 1])

with generate_col1:
    generate_button = st.button(
        "ğŸ”® Wygeneruj insighty",
        type="primary",
        use_container_width=True,
        help="Generuje raport AI na podstawie danych"
    )

with generate_col2:
    if st.button("ğŸ—‘ï¸ WyczyÅ›Ä‡ historiÄ™", use_container_width=True):
        st.session_state["insights_history"] = []
        st.success("âœ… Historia wyczyszczona")
        st.rerun()

if generate_button:
    start_time = time.time()
    
    # Progress
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # Etap 1: Przygotowanie
        status_text.text("ğŸ”„ PrzygotowujÄ™ dane...")
        progress_bar.progress(20)
        
        # PrÃ³bka dla AI (max 1000 wierszy dla szybkoÅ›ci)
        df_sample = df_main.head(1000) if len(df_main) > 1000 else df_main
        
        # Cache key
        cache_key = _create_cache_key(df_sample, prompt, config)
        
        # Etap 2: Generowanie
        status_text.text("ğŸ¤– GenerujÄ™ insighty z AI...")
        progress_bar.progress(40)
        
        raw_insights = _cached_generate_insights(cache_key, df_sample, prompt)
        
        # Etap 3: Przetwarzanie
        status_text.text("ğŸ“Š Przetwarzam odpowiedÅº...")
        progress_bar.progress(80)
        
        insight = _normalize_insights(raw_insights)
        
        # Etap 4: Zapisz do historii
        _add_to_history(insight)
        
        progress_bar.progress(100)
        elapsed = time.time() - start_time
        
        status_text.empty()
        progress_bar.empty()
        
        st.success(f"âœ… Wygenerowano w {elapsed:.1f}s")
        
        # ============================================================================
        # PREZENTACJA WYNIKÃ“W
        # ============================================================================
        
        st.divider()
        
        # Tab layout dla lepszej organizacji
        tab1, tab2, tab3 = st.tabs([
            "ğŸ“Š Raport", 
            "ğŸ’¾ Export", 
            "ğŸ” Debug"
        ])
        
        # TAB 1: RAPORT
        with tab1:
            # Summary
            st.markdown("### ğŸ§­ Executive Summary")
            st.info(insight.summary)
            
            # Top Insights
            if insight.top_insights:
                st.markdown("### ğŸ“Œ Top Insights")
                for i, item in enumerate(insight.top_insights, 1):
                    st.markdown(f"**{i}.** {item}")
            
            # Recommendations
            if insight.recommendations:
                st.markdown("### ğŸ’¡ Rekomendacje")
                for rec in insight.recommendations:
                    st.success(f"âœ“ {rec}")
            
            # Risks
            if insight.risks:
                st.markdown("### âš ï¸ Ryzyka i anomalie")
                for risk in insight.risks:
                    st.warning(f"âš  {risk}")
            
            # Metadata
            with st.expander("â„¹ï¸ Metadata raportu", expanded=False):
                st.json(insight.metadata)
        
        # TAB 2: EXPORT
        with tab2:
            st.subheader("ğŸ’¾ Eksportuj raport")
            
            col1, col2, col3 = st.columns(3)
            
            # JSON Export
            with col1:
                json_data = insight.to_json()
                st.download_button(
                    "â¬‡ï¸ Pobierz JSON",
                    data=json_data,
                    file_name=f"ai_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    use_container_width=True
                )
            
            # Markdown Export
            with col2:
                md_data = insight.to_markdown()
                st.download_button(
                    "â¬‡ï¸ Pobierz Markdown",
                    data=md_data,
                    file_name=f"ai_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown",
                    use_container_width=True
                )
            
            # TXT Export
            with col3:
                txt_data = insight.to_markdown()  # Same as MD but .txt extension
                st.download_button(
                    "â¬‡ï¸ Pobierz TXT",
                    data=txt_data,
                    file_name=f"ai_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain",
                    use_container_width=True
                )
            
            st.divider()
            
            # Preview exports
            with st.expander("ğŸ‘ï¸ PodglÄ…d Markdown", expanded=False):
                st.code(md_data, language="markdown")
        
        # TAB 3: DEBUG
        with tab3:
            if config.show_json:
                st.subheader("ğŸ” Surowy JSON")
                st.json(raw_insights)
            else:
                st.info("WÅ‚Ä…cz 'PokaÅ¼ surowy JSON' w ustawieniach, aby zobaczyÄ‡ debug info")
            
            st.subheader("âš™ï¸ Cache info")
            st.code(f"Cache key: {cache_key}")
        
    except Exception as e:
        progress_bar.empty()
        status_text.empty()
        
        st.error(f"âŒ BÅ‚Ä…d generowania insightÃ³w: {e}")
        logger.error(f"BÅ‚Ä…d generowania: {e}", exc_info=True)
        
        # Sugestie rozwiÄ…zania
        st.info(
            "ğŸ’¡ **MoÅ¼liwe rozwiÄ…zania:**\n"
            "- SprawdÅº poÅ‚Ä…czenie z internetem\n"
            "- Zweryfikuj klucz API w konfiguracji\n"
            "- SprÃ³buj trybu 'Szybka'\n"
            "- Odznacz 'UwzglÄ™dnij strukturÄ™ kolumn'"
        )

else:
    st.info(
        "ğŸ‘† Kliknij przycisk **Wygeneruj insighty**, aby rozpoczÄ…Ä‡ analizÄ™ AI.\n\n"
        f"Tryb: **{depth_option}** â€¢ "
        f"Schema: **{'âœ“' if config.include_schema else 'âœ—'}** â€¢ "
        f"Quality: **{'âœ“' if config.include_quality else 'âœ—'}**"
    )

# ========================================================================================
# HISTORIA INSIGHTÃ“W
# ========================================================================================

history = st.session_state.get("insights_history", [])

if history:
    st.divider()
    st.subheader("ğŸ“š Historia insightÃ³w")
    
    for idx, hist_item in enumerate(history):
        timestamp = hist_item.get("timestamp", "Unknown")
        summary = hist_item.get("summary", "")[:200] + "..."
        
        with st.expander(f"ğŸ•’ {timestamp}", expanded=(idx == 0)):
            # Rekonstrukcja AIInsight z dict
            hist_insight = AIInsight(**hist_item)
            
            st.markdown(f"**Summary:** {hist_insight.summary}")
            
            if hist_insight.top_insights:
                st.markdown("**Top Insights:**")
                for i, insight_text in enumerate(hist_insight.top_insights[:3], 1):
                    st.markdown(f"{i}. {insight_text}")
            
            # Quick export
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    "â¬‡ï¸ JSON",
                    data=hist_insight.to_json(),
                    file_name=f"history_{idx}.json",
                    mime="application/json",
                    key=f"hist_json_{idx}",
                    use_container_width=True
                )
            with col2:
                st.download_button(
                    "â¬‡ï¸ Markdown",
                    data=hist_insight.to_markdown(),
                    file_name=f"history_{idx}.md",
                    mime="text/markdown",
                    key=f"hist_md_{idx}",
                    use_container_width=True
                )

# ========================================================================================
# WSKAZÃ“WKI NAWIGACJI
# ========================================================================================

st.divider()
st.success(
    "âœ¨ **Co dalej?**\n\n"
    "- **ğŸ“Š EDA Analysis** â€” wizualizacje i statystyki\n"
    "- **ğŸ¯ Predictions** â€” trenowanie modeli ML\n"
    "- **ğŸ“ˆ Forecasting** â€” prognozy szeregÃ³w czasowych\n"
    "- **ğŸ“„ Reports** â€” generowanie raportÃ³w"
)